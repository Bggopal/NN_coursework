{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_FOR_XOR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Name:** Bala Guga Gopal S\n",
        "\n",
        "**Batch:**  R2\n",
        "\n",
        "**Date:** 31 Jan 2022\n",
        "\n",
        "**Experiment Name:**  8.4: Implement MLP for XOR logic (using MP neuron)\n",
        "\n",
        "**Experiment Description:** Implementing MLP for XOR logic (using MP neuron)"
      ],
      "metadata": {
        "id": "58AVFFjo5Kyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP for XOR logic\n",
        "A multilayer perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one.\n",
        "\n",
        "![image](https://www.researchgate.net/profile/M-Awadallah/publication/298918860/figure/fig5/AS:607381109096448@1521822349131/A-two-input-one-output-multi-layer-perceptron-feedforward-ANN.png)\n",
        "\n",
        "#XoR logic\n",
        "\n",
        "n XOR (exclusive OR gate) is a digital logic gate that gives a true output only when both its inputs differ from each other. The truth table for an XOR gate is shown below:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/59637425/152112045-000d3293-839c-449a-b8dd-6a453c0e62d3.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "18Gq9fFi5qNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "GNLFgUc9A4DM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cVxgWiMpSgqv"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation"
      ],
      "metadata": {
        "id": "db4Q-kM_A7bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "\treturn 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "def sigmoid_der(x):\n",
        "\treturn x*(1.0 - x)"
      ],
      "metadata": {
        "id": "LYqDhsDCmHCX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main function defintion\n",
        "l1 & l2 is layer 1 and layer 2,and wi and wh are weights"
      ],
      "metadata": {
        "id": "ss4CSz70Ap5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN:\n",
        "    def __init__(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.l=len(self.inputs)\n",
        "        self.li=len(self.inputs[0])\n",
        "\n",
        "        self.wi=np.random.random((self.li, self.l))\n",
        "        self.wh=np.random.random((self.l, 1))\n",
        "\n",
        "    def think(self, inp):\n",
        "        s1=sigmoid(np.dot(inp, self.wi))\n",
        "        s2=sigmoid(np.dot(s1, self.wh))\n",
        "        return s2\n",
        "\n",
        "    def train(self, inputs,outputs, it):\n",
        "        for i in range(it):\n",
        "            l0=inputs\n",
        "\n",
        "            #inside the preceptron\n",
        "            l1=sigmoid(np.dot(l0, self.wi))\n",
        "            l2=sigmoid(np.dot(l1, self.wh))\n",
        "\n",
        "            # Back propagation (output -> layer2)\n",
        "            l2_err=outputs - l2\n",
        "            l2_delta = np.multiply(l2_err, sigmoid_der(l2))\n",
        "\n",
        "            # Back propagation (layer2 -> layer1)\n",
        "            l1_err=np.dot(l2_delta, self.wh.T)\n",
        "            l1_delta=np.multiply(l1_err, sigmoid_der(l1))\n",
        "\n",
        "            #weight updates\n",
        "            self.wh+=np.dot(l1.T, l2_delta)\n",
        "            self.wi+=np.dot(l0.T, l1_delta)\n"
      ],
      "metadata": {
        "id": "y5yvdkxO5CjW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "2H3V1-yKBQwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=np.array([[0,0], [0,1], [1,0], [1,1] ])\n",
        "outputs=np.array([ [0], [1],[1],[0] ])\n",
        "\n",
        "n=NN(inputs)\n",
        "#print(n.think(inputs))\n",
        "n.train(inputs, outputs, 10000)\n",
        "print(np.round(n.think(inputs)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcOc_mILmQu-",
        "outputId": "0f36aafe-1dc1-47db-b5b2-b6b7787546d3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n"
          ]
        }
      ]
    }
  ]
}